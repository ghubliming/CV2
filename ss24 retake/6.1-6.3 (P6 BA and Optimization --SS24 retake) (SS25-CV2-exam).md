Let’s solve the problem step by step.

### 6.1 What are the expected values $\mu_1$, $\mu_2$ for $P(\hat{x}_1 | X_j) \sim \mathcal{N}(\mu_1, \Sigma_1)$ and $P(\hat{x}_2 | R, T, X_j) \sim \mathcal{N}(\mu_2, \Sigma_2)$ to model the noise in the observed 2D locations?

- $P(\hat{x}_1 | X_j) \sim \mathcal{N}(\mu_1, \Sigma_1)$ represents the probability density of the observed 2D point $\hat{x}_1$ given the 3D point $X_j$, modeled as a Gaussian with mean $\mu_1$ and covariance $\Sigma_1$. The mean $\mu_1$ is the expected value of $\hat{x}_1$, which corresponds to the projection $\pi(X_j)$ of the 3D point $X_j$ onto the 2D plane, assuming no noise. Thus, $\mu_1 = \pi(X_j)$.
- $P(\hat{x}_2 | R, T, X_j) \sim \mathcal{N}(\mu_2, \Sigma_2)$ represents the probability density of the observed 2D point $\hat{x}_2$ given the camera transformation $(R, T)$ and 3D point $X_j$, modeled as a Gaussian with mean $\mu_2$ and covariance $\Sigma_2$. The mean $\mu_2$ is the expected value of $\hat{x}_2$, which is the projection of the transformed 3D point $R X_j + T$ onto the 2D plane. Thus, $\mu_2 = \pi(R X_j + T)$.

**Answer:** $\mu_1 = \pi(X_j)$, $\mu_2 = \pi(R X_j + T)$.

For 6.1, using $x_j$ instead of $\pi(X_j)$ for $\mu_1$ is not appropriate unless $x_j$ is specifically defined as the projected 2D coordinates of the 3D point $X_j$ under the projection function $\pi$. Let’s clarify:

- The problem defines $\pi: \mathbb{R}^3 \to \mathbb{R}^2$ as the projection function mapping 3D points $X_j$ to 2D coordinates. The expected value $\mu_1$ for $P(\hat{x}_1 | X_j) \sim \mathcal{N}(\mu_1, \Sigma_1)$ represents the noiseless projection of $X_j$ onto the 2D plane, which is $\pi(X_j)$.
- If $x_j$ is intended to represent the 2D projection $\pi(X_j)$ (e.g., as a notation choice in your context), then you could use $x_j$ instead, provided it’s consistent. However, the problem’s notation uses $\hat{x}_1$ and $\hat{x}_2$ for observed 2D points and $\pi$ for the projection, suggesting $x_j$ (if used) might refer to something else, like the 3D point itself or a different variable.

To be precise and align with the problem statement, you should use $\mu_1 = \pi(X_j)$. Substituting $x_j$ for $\pi(X_j)$ is only valid if $x_j$ is explicitly defined as the projected 2D point. Without that definition, using $x_j$ could introduce ambiguity or error.

**Answer:** No, you should use $\pi(X_j)$ unless $x_j$ is explicitly defined as $\pi(X_j)$ in your context.

### 6.2 What is the cost function for the bundle adjustment problem in terms of $\mu_1$ and $\mu_2$?

The goal of bundle adjustment is to find the 3D point positions $X_j$ and camera parameters ($R, T$) that are most likely, given the observed 2D points $x_j^1$ and $x_j^2$. This is a Maximum Likelihood Estimation (MLE) problem, as stated in equation (6.1):

$$
\arg\max_{R, T, \{X_j\}} \prod_{j=1}^{N} P(x_j^1 | X_j) P(x_j^2 | R, T, X_j)
$$

To make this optimization problem tractable, we convert the product of probabilities into a sum by taking the logarithm (log-likelihood). Maximizing the likelihood is equivalent to maximizing the log-likelihood:

$$
\arg\max_{R, T, \{X_j\}} \sum_{j=1}^{N} \left( \log P(x_j^1 | X_j) + \log P(x_j^2 | R, T, X_j) \right)
$$

The probability density function for a multivariate Gaussian distribution $\mathcal{N}(\mu, \Sigma)$ is:
$$
P(x) = \frac{1}{\sqrt{(2\pi)^k |\Sigma|}} \exp\left(-\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu)\right)
$$
The logarithm is:
$$
\log P(x) = -\frac{1}{2}(x-\mu)^T \Sigma^{-1} (x-\mu) - \frac{k}{2}\log(2\pi) - \frac{1}{2}\log|\Sigma|
$$

Maximizing the log-likelihood is equivalent to **minimizing the negative log-likelihood**. When we negate the expression above, the constant terms can be ignored for the optimization, as they don't depend on the parameters we are optimizing ($R, T, \{X_j\}$). This leaves us with the core of the cost function:

$$
\arg\min_{R, T, \{X_j\}} \sum_{j=1}^{N} \left( (x_j^1 - \mu_1)^T \Sigma_1^{-1} (x_j^1 - \mu_1) + (x_j^2 - \mu_2)^T \Sigma_2^{-1} (x_j^2 - \mu_2) \right)
$$

This is the sum of squared Mahalanobis distances.

If we assume the noise is isotropic and has the same variance in all directions (i.e., $\Sigma = \sigma^2 I$), the cost function simplifies to the sum of squared Euclidean distances, which is the standard **reprojection error**:

$$
C(R, T, \{X_j\}) = \sum_{j=1}^{N} \left( \|x_j^1 - \mu_1\|^2 + \|x_j^2 - \mu_2\|^2 \right)
$$

Substituting the expressions for $\mu_1$ and $\mu_2$ from part 6.1, we get the full cost function for bundle adjustment:

$$
C(R, T, \{X_j\}) = \sum_{j=1}^{N} \left( \|x_j^1 - \pi(X_j)\|^2 + \|x_j^2 - \pi(R, T, X_j)\|^2 \right)
$$

Therefore, the term inside the summation for equation (6.2) is the sum of the squared reprojection errors for the two cameras, expressed in terms of $\mu_1$ and $\mu_2$:

$$
\|x_j^1 - \mu_1\|^2 + \|x_j^2 - \mu_2\|^2
$$

### 6.3 Is solving (6.1) in general equivalent to minimizing the bundle adjustment cost function? If not, when is it equivalent?

- Equation (6.1) involves maximizing the product $\prod_{j=1}^N P(\hat{x}_1 | X_j) P(\hat{x}_2 | R, T, X_j)$, where $P$ are probability density functions modeled as Gaussians.
- Maximizing the product of Gaussian likelihoods is equivalent to minimizing the negative log-likelihood, which leads to the cost function derived in 6.2 (up to a constant).
- However, this equivalence holds only if the noise models are Gaussian and the covariances $\Sigma_1$ and $\Sigma_2$ are known and positive definite. If the noise distributions are non-Gaussian or the covariances are not well-defined, the maximization of (6.1) may not correspond to minimizing the standard bundle adjustment cost function.
- Additionally, the problem assumes the projection function $\pi$ and the camera parameters $(R, T)$ are correctly modeled. Any nonlinearity or approximation in $\pi$ could affect the equivalence.

**Answer:** Solving (6.1) is equivalent to minimizing the bundle adjustment cost function when the noise is Gaussian with known, positive definite covariances $\Sigma_1$ and $\Sigma_2$, and the projection function $\pi$ is accurately modeled. Otherwise, it is not equivalent.

--- 
